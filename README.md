# Stanford Alpaca: An Instruction-following LLaMA Model

This repo is an unofficial implementation for Stanford's Alpaca models that support full-parameter finetune, LoRA and QLoRA, based on [standford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), [alpaca-lora](https://github.com/tloen/alpaca-lora).

## Overview
For more information about Alpaca amd LLaMA, please read the original documents.

[0]: Alpaca: A Strong, Replicable Instruction-Following Model. Rohan Taori*, Ishaan Gulrajani*, Tianyi Zhang*, Yann Dubois*, Xuechen Li*, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. https://crfm.stanford.edu/2023/03/13/alpaca.html

[1]: LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. https://arxiv.org/abs/2302.13971v1

##



## Author
- [Huawei Lin](https://huaweilin.net/)



